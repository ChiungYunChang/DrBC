{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTf6T_V-SDS7"
      },
      "source": [
        "## Assignment 1 : Learning to Identify High BC Nodes\n",
        "\n",
        "Implement the **DrBC** approach from ***Learning to Identify High Betweenness Centrality Nodes from Scratch: A Novel Graph Neural Network Approach***.  paper : https://arxiv.org/pdf/1905.10418.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLHxopXPTa6"
      },
      "source": [
        "## Download Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKeKY2J7PD1Z",
        "outputId": "36af2481-2aa9-41bf-bde9-534c0703216d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 48 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 407 kB 4.1 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7YoIHpeAKO7"
      },
      "source": [
        "## Import Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zspciFpXPd-R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "import community\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iYEkdXj1GST8",
        "outputId": "cafd5dee-c15d-4002-98e6-94bf3b294a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.7-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 30.4 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.8 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=72cd46cd38ac55bfc62431ad9cedf4f65b18f25c7c22cf4181ff3f6412fb3dfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.7 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.11 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIOXHIb_JZs"
      },
      "source": [
        "## Create Graph \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyTzs8LaIUt"
      },
      "source": [
        "Using  ```nx.random_graphs.powerlaw_cluster_graph``` to generate the graph and for training and `nx.betweenness_centrality ` is to calculate the betweenness centrality of each node.\n",
        "For message passing, we need **edge**,\n",
        "**node** information. \n",
        "\n",
        "* **edge_index** : Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
        "* **Generating a synthetic graph for training**\n",
        "  * Generating graphs by the ***power‐law cluster model*** with n=“number of nodes”, m=4, p=0.05\n",
        " * networkx.generators.random_graphs.powerlaw_cluster_graph\n",
        "\n",
        "* In this paper they randomly sampled 5|V | source nodes and 5|V |\n",
        "target nodes with replacement, forming 5|V | random node pairs to\n",
        "compute the loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7vfrXBzk_HY3"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "batch_size = 5\n",
        "class Graph(): \n",
        "  def __init__(self, batch_size):\n",
        "    self.graph_data = []\n",
        "    # generate the graph with power-law distribution\n",
        "    for i in range(batch_size):\n",
        "      G = nx.random_graphs.powerlaw_cluster_graph(random.randint(500,800), 4, 0.05)\n",
        "      self.graph_data.append(G)\n",
        "      \n",
        "  # calculate node degree\n",
        "  def node_degree(self):\n",
        "    degree_list = []\n",
        "    for g in self.graph_data:\n",
        "      for n in range(g.number_of_nodes()):\n",
        "        degree_list.append([g.degree[n],1,1])\n",
        "    return torch.Tensor(degree_list)\n",
        "  \n",
        "  # calculate edge index \n",
        "  def get_edge_index(self):\n",
        "    start_node, end_node, node_number = [],[],0\n",
        "    for graph in self.graph_data:\n",
        "      for edge in graph.edges():\n",
        "        start,end = edge\n",
        "        start_node.append(start + node_number)\n",
        "        end_node.append(end+ node_number)\n",
        "      node_number += graph.number_of_nodes()\n",
        "    # bidirection edge \n",
        "    edge_index=[start_node+end_node, end_node+start_node]\n",
        "    return torch.LongTensor(edge_index) \n",
        "\n",
        "  # calculate betweeness centrality \n",
        "  def calculate_bc(self):\n",
        "    bc_list = [list(nx.betweenness_centrality(graph).values()) for graph in self.graph_data]\n",
        "    labels = []\n",
        "    for bc in bc_list:\n",
        "        labels.extend(bc)\n",
        "    log_labels = [-math.log(v+1e-8) for v in labels] \n",
        "    return torch.Tensor(log_labels)\n",
        "\n",
        "  # Using random node pairs to compute the loss\n",
        "  def get_pairs(self, repeat=5):\n",
        "    id_nums = 0;source_ids = [];target_ids = []    \n",
        "    for graph in self.graph_data:\n",
        "      node_nums = len(graph.nodes)\n",
        "      source_id = [i for i in range(id_nums, node_nums+id_nums)]\n",
        "      target_id = [i for i in range(id_nums, node_nums+id_nums)]\n",
        "      # sample 5|V| source nodes and 5|V| target nodes 5|V|\n",
        "      source_id *= 5\n",
        "      target_id *= 5\n",
        "      random.shuffle(source_id)\n",
        "      random.shuffle(target_id)\n",
        "      source_ids.extend(source_id)\n",
        "      target_ids.extend(target_id)\n",
        "    return source_ids, target_ids\n",
        "\n",
        "G = Graph(10)\n",
        "# print(G.calculate_bc())\n",
        "# print(np.shape(G.calculate_bc()))\n",
        "# print(G.get_edge_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCQNS2u_mEF"
      },
      "source": [
        "## Implementing the GCN Layer\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtvDI1--ZsBW"
      },
      "source": [
        "#### Step 1 : Message passing \n",
        "\n",
        "   ![image.png](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_message_passing.svg?raw=1)\n",
        "\n",
        "Reference code: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0JcUDip7_vzw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Step 1: Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        # Step 2: Linearly transform node feature matrix.\n",
        "        x = self.lin(x)\n",
        "        # Step 3: Compute normalization\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        # Step 4-6: Start propagating messages.\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # x_j has shape [E, out_channels]\n",
        "        # Step 4: Normalize node features.\n",
        "        return norm.view(-1, 1) * x_j\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS1zDGT5WL5w"
      },
      "source": [
        "#### Step 2 : Implement DrBC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMmjInbH6wG4"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn as nn\n",
        "import torch \n",
        "\n",
        "class Encoder(nn.Module) : \n",
        "  def __init__(self, input_dim=3, embedding_dim=128):\n",
        "    super(Encoder, self).__init__()     \n",
        "    self.fc = nn.Linear(input_dim, embedding_dim)\n",
        "    self.relu = nn.LeakyReLU()\n",
        "         \n",
        "    self.gcn1 = GCNConv(128, 128) # message passing\n",
        "    self.gru1 = nn.GRU(128, 128)\n",
        "\n",
        "    self.gcn2 = GCNConv(128, 128) \n",
        "    self.gru2 = nn.GRU(128, 128)\n",
        "\n",
        "    self.gcn3 = GCNConv(128, 128) \n",
        "    self.gru3 = nn.GRU(128, 128)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    outs = []\n",
        "    x = self.fc(x)\n",
        "    x = self.relu(x)        \n",
        "    gcnx = self.gcn1(x, edge_index)\n",
        "    x1, _ = self.gru1(gcnx.view(1, *gcnx.shape), x.view(1, *x.shape))\n",
        "    gcnx = self.gcn2(x1[0], edge_index)\n",
        "    x2, _ = self.gru2(gcnx.view(1, *gcnx.shape), x1)\n",
        "    gcnx = self.gcn3(x2[0], edge_index)\n",
        "    x3, _ = self.gru3(gcnx.view(1, *gcnx.shape), x2)\n",
        "   \n",
        "    # Layer Aggregator : Max Pooling\n",
        "    outs = torch.stack([x1[0],x2[0],x3[0]])\n",
        "    max_outs = torch.max(outs, dim=0).values\n",
        "    return max_outs\n",
        "\n",
        "class Decoder(nn.Module) :\n",
        "  def __init__(self, input_dim=64, hidden_dim=32):\n",
        "    super(Decoder, self).__init__()\n",
        "    # Decoder\n",
        "    self.fc1 = nn.Linear(128, hidden_dim)\n",
        "    self.relu1 = nn.LeakyReLU() \n",
        "    self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu2 = nn.LeakyReLU() \n",
        "    self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DrBC_model(nn.Module):\n",
        "  def __init__(self, input_dim=3, embedding_dim=128):\n",
        "    super(DrBC_model, self).__init__()\n",
        "    self.encoder = Encoder(input_dim, embedding_dim)\n",
        "    self.decoder = Decoder(input_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    en_x = self.encoder(x,edge_index)\n",
        "    outs = self.decoder(en_x)\n",
        "    return outs\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6pHjems_yVy",
        "outputId": "0538e59c-873d-4ac7-a2fb-9ad8a6b99ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0037],\n",
            "        [0.0098],\n",
            "        [0.0101],\n",
            "        [0.0167]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = DrBC_model()\n",
        "# test \n",
        "x = torch.tensor([[2, 1, 1],[2, 1, 1],[2, 1, 1],[2, 1, 1]],dtype=torch.float)\n",
        "edge_index = torch.tensor([[0,1,2],[1,2,3]],dtype=torch.long)\n",
        "model(x, edge_index)\n",
        "print(model(x, edge_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S-BZI90z3-u8"
      },
      "outputs": [],
      "source": [
        "def loss_function(out, bc_value, source_ids, target_ids):\n",
        "  pred = out[source_ids] - out[target_ids]\n",
        "  gt = torch.sigmoid((bc_value[source_ids] - bc_value[target_ids]))\n",
        "  gt = gt.view(-1, 1)\n",
        "  loss = F.binary_cross_entropy_with_logits(pred, gt, reduction=\"sum\")\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "HO3Zw1lC_3Zn",
        "outputId": "4dfb79ed-2a56-4d75-c7c9-677c3bdb4ca7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220312_090748-2re5vd6r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/imhilaryy/DrBC/runs/2re5vd6r\" target=\"_blank\">cerulean-valley-1</a></strong> to <a href=\"https://wandb.ai/imhilaryy/DrBC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4641.61083984375\n",
            "Validation Loss Decreased(inf--->4641.610840) \t Saving The Model\n",
            "[0/2000] Loss:23062.1055\n",
            " Validation Loss: 3742.56103515625\n",
            "Validation Loss Decreased(4641.610840--->3742.561035) \t Saving The Model\n",
            "[100/2000] Loss:15520.5996\n",
            " Validation Loss: 3753.897216796875\n",
            "[200/2000] Loss:15164.7031\n",
            " Validation Loss: 3736.465087890625\n",
            "Validation Loss Decreased(3742.561035--->3736.465088) \t Saving The Model\n",
            "[300/2000] Loss:15145.2793\n",
            " Validation Loss: 3740.455810546875\n",
            "[400/2000] Loss:16381.2461\n",
            " Validation Loss: 3743.732177734375\n",
            "[500/2000] Loss:16343.1650\n",
            " Validation Loss: 3739.553466796875\n",
            "[600/2000] Loss:14829.7676\n",
            " Validation Loss: 3751.727783203125\n",
            "[700/2000] Loss:14831.1934\n",
            " Validation Loss: 3751.43212890625\n",
            "[800/2000] Loss:16837.3027\n",
            " Validation Loss: 3724.510986328125\n",
            "Validation Loss Decreased(3736.465088--->3724.510986) \t Saving The Model\n",
            "[900/2000] Loss:16784.3340\n",
            " Validation Loss: 3747.75146484375\n",
            "[1000/2000] Loss:16538.8223\n",
            " Validation Loss: 3758.32666015625\n",
            "[1100/2000] Loss:16463.3672\n",
            " Validation Loss: 3739.948486328125\n",
            "[1200/2000] Loss:16025.0469\n",
            " Validation Loss: 3729.6845703125\n",
            "[1300/2000] Loss:15984.5674\n",
            " Validation Loss: 3812.680419921875\n",
            "[1400/2000] Loss:14716.5967\n",
            " Validation Loss: 3723.379150390625\n",
            "Validation Loss Decreased(3724.510986--->3723.379150) \t Saving The Model\n",
            "[1500/2000] Loss:14603.7266\n",
            " Validation Loss: 3751.2099609375\n",
            "[1600/2000] Loss:14936.8535\n",
            " Validation Loss: 3737.77880859375\n",
            "[1700/2000] Loss:14863.9258\n",
            " Validation Loss: 3755.6669921875\n",
            "[1800/2000] Loss:16427.4277\n",
            " Validation Loss: 3728.6298828125\n",
            "[1900/2000] Loss:16377.6260\n"
          ]
        }
      ],
      "source": [
        "from networkx.algorithms.centrality.degree_alg import out_degree_centrality\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "def train():\n",
        "    model = DrBC_model()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define optimizer.\n",
        "    # model = model.cuda()   \n",
        "    eval_graph = Graph(batch_size=2)\n",
        "    evsl_bc_value = eval_graph.calculate_bc()\n",
        "    min_valid_loss = np.inf\n",
        "    iternum = 2000\n",
        "    wandb.init(project=\"DrBC\")\n",
        "    for iter in range(iternum):\n",
        "      optimizer.zero_grad()\n",
        "      if iter % 200 == 0 :\n",
        "        graph = Graph(10)  \n",
        "        bc_value = graph.calculate_bc()\n",
        "      out = model(graph.node_degree(), graph.get_edge_index())\n",
        "      source_ids, target_ids = graph.get_pairs()\n",
        "      loss = loss_function(out, bc_value, source_ids, target_ids)\n",
        "      wandb.log({'pair_loss': loss, 'iter': iter}) # record trsining loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # validation\n",
        "      if iter % 100 == 0: \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          out = model(eval_graph.node_degree(), eval_graph.get_edge_index())\n",
        "        source_ids, target_ids = eval_graph.get_pairs()\n",
        "        vali_loss = loss_function(out, evsl_bc_value, source_ids, target_ids)   \n",
        "        print(f' Validation Loss: {vali_loss}')\n",
        "        wandb.log({'validation_loss': vali_loss, 'iter': iter})\n",
        "        if min_valid_loss > vali_loss:\n",
        "          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{vali_loss:.6f}) \\t Saving The Model')\n",
        "          min_valid_loss = vali_loss\n",
        "          # Saving State Dict\n",
        "          torch.save(model.state_dict(), 'saved_model.pth')       \n",
        "        print(\"[{}/{}] Loss:{:.4f}\".format(iter, iternum, loss.item()))             \n",
        "    return model\n",
        "\n",
        "model = train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkcPsMiYxSwy"
      },
      "source": [
        "## Testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXatl4L4J4cI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class test_data():\n",
        "    def __init__(self):\n",
        "        super(test_data, self).__init__()\n",
        "        self.TestGraph_feature = []\n",
        "        self.data = []\n",
        "        self.edge_list = []\n",
        "        self.BC = []\n",
        "        self.TestGraph_edge = [[], []]  # TestGraph_edge[0]=source,[1]= target\n",
        "\n",
        "    def readfile(self, path):\n",
        "        data = []\n",
        "        file = open(path, \"r\")\n",
        "        for line in file.readlines():\n",
        "            data.append(line.split())\n",
        "        file.close()\n",
        "        return data\n",
        "\n",
        "    def get_num_node(self, score_path):\n",
        "        num_node = 0\n",
        "        # calculate node number\n",
        "        for i in score_path:\n",
        "            num_node += 1\n",
        "        print(num_node)\n",
        "        node_degree = [0] * num_node\n",
        "        return node_degree\n",
        "\n",
        "    def get_edge_node_index(self, edge_index, node_degree):\n",
        "        # initial node degree\n",
        "        start_edge = [];\n",
        "        end_edge = []\n",
        "        print(np.shape(edge_index))\n",
        "        for [s, t] in edge_index:\n",
        "            # get node degree\n",
        "            node_degree[int(s)] += 1\n",
        "            node_degree[int(t)] += 1\n",
        "            # bidirectional edge\n",
        "            start_edge += [int(s)]\n",
        "            end_edge += [int(t)]\n",
        "        # node extension : bidirectional edge\n",
        "        for nb in node_degree:\n",
        "            self.TestGraph_feature.append([nb, 1, 1])\n",
        "        self.TestGraph_edge[0].extend(start_edge)\n",
        "        self.TestGraph_edge[1].extend(end_edge)\n",
        "        self.TestGraph_edge[0].extend(end_edge)\n",
        "        self.TestGraph_edge[1].extend(start_edge)\n",
        "        return self.TestGraph_edge, self.TestGraph_feature\n",
        "\n",
        "    # betweenness centrality value\n",
        "    def get_BC(self, file_path):\n",
        "\n",
        "        gt = self.readfile(file_path)\n",
        "        print(\"BC\",np.shape(gt))\n",
        "        for (node_id, bc) in gt:\n",
        "            bc = -math.log(float(bc) + 1e-8)\n",
        "            self.BC.append([bc])\n",
        "        return self.BC\n",
        "\n",
        "    def topN_accuracy(self, outs, label):\n",
        "        topk = [1, 5, 10]\n",
        "        k_accuracy = []\n",
        "        node_nums = len(outs)\n",
        "\n",
        "        label = label.reshape(-1)\n",
        "        outs = outs.reshape(-1)\n",
        "        tau, _ = stats.kendalltau(outs.cpu().detach().numpy(), label.cpu().detach().numpy())\n",
        "        print(\"Kendall tau: {}\".format(tau))\n",
        "        label = torch.argsort(label)\n",
        "        outs = torch.argsort(outs)\n",
        "\n",
        "        for k in topk:\n",
        "            k_num = int(node_nums * k / 100)\n",
        "            k_label = label[:k_num].tolist()\n",
        "            k_outs = outs[:k_num].tolist()\n",
        "\n",
        "            correct = list(set(k_label) & set(k_outs))\n",
        "            accuracy = len(correct) / (k_num)\n",
        "            k_accuracy.append(accuracy)\n",
        "            print(\"Top-{} accuracy: {}\".format(k, accuracy * 100))\n",
        "\n",
        "        return k_accuracy,tau\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def testing(PATH, Ground_truthPath):\n",
        "    testing = test_data()\n",
        "    model = DrBC_model()\n",
        "    model.load_state_dict(torch.load('model/500800.pth'))\n",
        "    # predictbc_value = torch.tensor(testing.get_BC(PATH))\n",
        "    ground_trueBC = torch.tensor(testing.get_BC(Ground_truthPath))\n",
        "    # print(ground_trueBC)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # ground truth\n",
        "        dataset = testing.readfile(PATH)\n",
        "        dataset_score = testing.readfile(Ground_truthPath)\n",
        "        node_degree_data = testing.get_num_node(dataset_score)\n",
        "        TestGraph_edge_index, TestGraph_feature = testing.get_edge_node_index(dataset, node_degree_data)\n",
        "        print(\"DATA DONE\")\n",
        "        # model = model.cpu()\n",
        "        outs = model(torch.FloatTensor(TestGraph_feature).cuda(), torch.tensor(TestGraph_edge_index).cuda())\n",
        "        k_accuracy ,tau= testing.topN_accuracy(outs, ground_trueBC )\n",
        "    return k_accuracy,tau\n",
        "\n",
        "# file path\n",
        "yt_node_pair_file = './youtube/com-youtube.txt'\n",
        "yt_bc_score_file = './youtube/com-youtube_score.txt'\n",
        "Synthetic_graph_root = \"Synthetic/5000/\"\n",
        "Synthetic_score = \"Synthetic/5000/\"\n",
        "# youtube graph\n",
        "# print(testing(yt_node_pair_file, yt_bc_score_file))\n",
        "# Synthetic graph\n",
        "ACC = []\n",
        "k =[]\n",
        "for id in range(30):\n",
        "    Synthetic_graph = Synthetic_graph_root + f'{id}.txt'\n",
        "    Synthetic_score = Synthetic_graph_root + f'{id}_score.txt'\n",
        "    k_accuracy,tau= testing(Synthetic_graph, Synthetic_score)\n",
        "    ACC.append(k_accuracy)\n",
        "    k.append(tau)\n",
        "ACC = np.array(ACC)\n",
        "average = np.mean(ACC,axis=0)\n",
        "k= np.mean(k)\n",
        "print(average,\"TTTT\", \"k\",k)\n"
      ],
      "metadata": {
        "id": "8Y_11eda-vuC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment 1 : Learning to Identify High BC Nodes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
