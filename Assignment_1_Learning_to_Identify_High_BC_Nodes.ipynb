{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTf6T_V-SDS7"
      },
      "source": [
        "## Assignment 1 : Learning to Identify High BC Nodes\n",
        "\n",
        "Implement the **DrBC** approach from ***Learning to Identify High Betweenness Centrality Nodes from Scratch: A Novel Graph Neural Network Approach***.  paper : https://arxiv.org/pdf/1905.10418.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLHxopXPTa6"
      },
      "source": [
        "## Download Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zKeKY2J7PD1Z"
      },
      "outputs": [],
      "source": [
        "# Install torch geometric\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7YoIHpeAKO7"
      },
      "source": [
        "## Import Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zspciFpXPd-R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "import community\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEkdXj1GST8",
        "outputId": "1b8514cc-f01b-4c33-e4a8-53115ebc8708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.11)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.7)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIOXHIb_JZs"
      },
      "source": [
        "## Create Graph \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyTzs8LaIUt"
      },
      "source": [
        "Using  ```nx.random_graphs.powerlaw_cluster_graph``` to generate the graph and for training and `nx.betweenness_centrality ` is to calculate the betweenness centrality of each node.\n",
        "For message passing, we need **edge**,\n",
        "**node** information. \n",
        "\n",
        "* **edge_index** : Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
        "* **Generating a synthetic graph for training**\n",
        "  * Generating graphs by the ***power‐law cluster model*** with n=“number of nodes”, m=4, p=0.05\n",
        " * networkx.generators.random_graphs.powerlaw_cluster_graph\n",
        "\n",
        "* In this paper they randomly sampled 5|V | source nodes and 5|V |\n",
        "target nodes with replacement, forming 5|V | random node pairs to\n",
        "compute the loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7vfrXBzk_HY3"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "batch_size = 5\n",
        "class Graph(): \n",
        "  def __init__(self, batch_size):\n",
        "    self.graph_data = []\n",
        "    # generate the graph with power-law distribution\n",
        "    for i in range(batch_size):\n",
        "      G = nx.random_graphs.powerlaw_cluster_graph(random.randint(500,800), 4, 0.05)\n",
        "      self.graph_data.append(G)\n",
        "      \n",
        "  # calculate node degree\n",
        "  def node_degree(self):\n",
        "    degree_list = []\n",
        "    for g in self.graph_data:\n",
        "      for n in range(g.number_of_nodes()):\n",
        "        degree_list.append([g.degree[n],1,1])\n",
        "    return torch.Tensor(degree_list)\n",
        "  \n",
        "  # calculate edge index \n",
        "  def get_edge_index(self):\n",
        "    start_node, end_node, node_number = [],[],0\n",
        "    for graph in self.graph_data:\n",
        "      for edge in graph.edges():\n",
        "        start,end = edge\n",
        "        start_node.append(start + node_number)\n",
        "        end_node.append(end+ node_number)\n",
        "      node_number += graph.number_of_nodes()\n",
        "    # bidirection edge \n",
        "    edge_index=[start_node+end_node, end_node+start_node]\n",
        "    return torch.LongTensor(edge_index) \n",
        "\n",
        "  # calculate betweeness centrality \n",
        "  def calculate_bc(self):\n",
        "    bc_list = [list(nx.betweenness_centrality(graph).values()) for graph in self.graph_data]\n",
        "    labels = []\n",
        "    for bc in bc_list:\n",
        "        labels.extend(bc)\n",
        "    log_labels = [-math.log(v+1e-8) for v in labels] \n",
        "    return torch.Tensor(log_labels)\n",
        "\n",
        "  # Using random node pairs to compute the loss\n",
        "  def get_pairs(self, repeat=5):\n",
        "    id_nums = 0;source_ids = [];target_ids = []    \n",
        "    for graph in self.graph_data:\n",
        "      node_nums = len(graph.nodes)\n",
        "      source_id = [i for i in range(id_nums, node_nums+id_nums)]\n",
        "      target_id = [i for i in range(id_nums, node_nums+id_nums)]\n",
        "      # sample 5|V| source nodes and 5|V| target nodes 5|V|\n",
        "      source_id *= 5\n",
        "      target_id *= 5\n",
        "      random.shuffle(source_id)\n",
        "      random.shuffle(target_id)\n",
        "      source_ids.extend(source_id)\n",
        "      target_ids.extend(target_id)\n",
        "    return source_ids, target_ids\n",
        "\n",
        "G = Graph(10)\n",
        "# print(G.calculate_bc())\n",
        "# print(np.shape(G.calculate_bc()))\n",
        "# print(G.get_edge_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCQNS2u_mEF"
      },
      "source": [
        "## Implementing the GCN Layer\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtvDI1--ZsBW"
      },
      "source": [
        "#### Step 1 : Message passing \n",
        "\n",
        "   ![image.png](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_message_passing.svg?raw=1)\n",
        "\n",
        "Reference code: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0JcUDip7_vzw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Step 1: Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        # Step 2: Linearly transform node feature matrix.\n",
        "        x = self.lin(x)\n",
        "        # Step 3: Compute normalization\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        # Step 4-6: Start propagating messages.\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # x_j has shape [E, out_channels]\n",
        "        # Step 4: Normalize node features.\n",
        "        return norm.view(-1, 1) * x_j\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS1zDGT5WL5w"
      },
      "source": [
        "#### Step 2 : Implement DrBC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zMmjInbH6wG4"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn as nn\n",
        "import torch \n",
        "\n",
        "class Encoder(nn.Module) : \n",
        "  def __init__(self, input_dim=3, embedding_dim=128):\n",
        "    super(Encoder, self).__init__()     \n",
        "    self.fc = nn.Linear(input_dim, embedding_dim)\n",
        "    self.relu = nn.LeakyReLU()\n",
        "         \n",
        "    self.gcn1 = GCNConv(128, 128) # message passing\n",
        "    self.gru1 = nn.GRU(128, 128)\n",
        "\n",
        "    self.gcn2 = GCNConv(128, 128) \n",
        "    self.gru2 = nn.GRU(128, 128)\n",
        "\n",
        "    self.gcn3 = GCNConv(128, 128) \n",
        "    self.gru3 = nn.GRU(128, 128)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    outs = []\n",
        "    x = self.fc(x)\n",
        "    x = self.relu(x)        \n",
        "    gcnx = self.gcn1(x, edge_index)\n",
        "    x1, _ = self.gru1(gcnx.view(1, *gcnx.shape), x.view(1, *x.shape))\n",
        "    gcnx = self.gcn2(x1[0], edge_index)\n",
        "    x2, _ = self.gru2(gcnx.view(1, *gcnx.shape), x1)\n",
        "    gcnx = self.gcn3(x2[0], edge_index)\n",
        "    x3, _ = self.gru3(gcnx.view(1, *gcnx.shape), x2)\n",
        "   \n",
        "    # Layer Aggregator : Max Pooling\n",
        "    outs = torch.stack([x1[0],x2[0],x3[0]])\n",
        "    max_outs = torch.max(outs, dim=0).values\n",
        "    return max_outs\n",
        "\n",
        "class Decoder(nn.Module) :\n",
        "  def __init__(self, input_dim=64, hidden_dim=32):\n",
        "    super(Decoder, self).__init__()\n",
        "    # Decoder\n",
        "    self.fc1 = nn.Linear(128, hidden_dim)\n",
        "    self.relu1 = nn.LeakyReLU() \n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.relu2 = nn.LeakyReLU() \n",
        "    self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DrBC_model(nn.Module):\n",
        "  def __init__(self, input_dim=3, embedding_dim=128):\n",
        "    super(DrBC_model, self).__init__()\n",
        "    self.encoder = Encoder(input_dim, embedding_dim)\n",
        "    self.decoder = Decoder(input_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    en_x = self.encoder(x,edge_index)\n",
        "    outs = self.decoder(en_x)\n",
        "    return outs\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6pHjems_yVy",
        "outputId": "0538e59c-873d-4ac7-a2fb-9ad8a6b99ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0037],\n",
            "        [0.0098],\n",
            "        [0.0101],\n",
            "        [0.0167]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = DrBC_model()\n",
        "# test \n",
        "x = torch.tensor([[2, 1, 1],[2, 1, 1],[2, 1, 1],[2, 1, 1]],dtype=torch.float)\n",
        "edge_index = torch.tensor([[0,1,2],[1,2,3]],dtype=torch.long)\n",
        "model(x, edge_index)\n",
        "print(model(x, edge_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-BZI90z3-u8"
      },
      "outputs": [],
      "source": [
        "def loss_function(out, bc_value, source_ids, target_ids):\n",
        "  pred = out[source_ids] - out[target_ids]\n",
        "  gt = torch.sigmoid((bc_value[source_ids] - bc_value[target_ids]))\n",
        "  gt = gt.view(-1, 1)\n",
        "  loss = F.binary_cross_entropy_with_logits(pred, gt, reduction=\"sum\")\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "HO3Zw1lC_3Zn",
        "outputId": "4dfb79ed-2a56-4d75-c7c9-677c3bdb4ca7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220312_090748-2re5vd6r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/imhilaryy/DrBC/runs/2re5vd6r\" target=\"_blank\">cerulean-valley-1</a></strong> to <a href=\"https://wandb.ai/imhilaryy/DrBC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 4641.61083984375\n",
            "Validation Loss Decreased(inf--->4641.610840) \t Saving The Model\n",
            "[0/2000] Loss:23062.1055\n",
            " Validation Loss: 3742.56103515625\n",
            "Validation Loss Decreased(4641.610840--->3742.561035) \t Saving The Model\n",
            "[100/2000] Loss:15520.5996\n",
            " Validation Loss: 3753.897216796875\n",
            "[200/2000] Loss:15164.7031\n",
            " Validation Loss: 3736.465087890625\n",
            "Validation Loss Decreased(3742.561035--->3736.465088) \t Saving The Model\n",
            "[300/2000] Loss:15145.2793\n",
            " Validation Loss: 3740.455810546875\n",
            "[400/2000] Loss:16381.2461\n",
            " Validation Loss: 3743.732177734375\n",
            "[500/2000] Loss:16343.1650\n",
            " Validation Loss: 3739.553466796875\n",
            "[600/2000] Loss:14829.7676\n",
            " Validation Loss: 3751.727783203125\n",
            "[700/2000] Loss:14831.1934\n",
            " Validation Loss: 3751.43212890625\n",
            "[800/2000] Loss:16837.3027\n",
            " Validation Loss: 3724.510986328125\n",
            "Validation Loss Decreased(3736.465088--->3724.510986) \t Saving The Model\n",
            "[900/2000] Loss:16784.3340\n",
            " Validation Loss: 3747.75146484375\n",
            "[1000/2000] Loss:16538.8223\n",
            " Validation Loss: 3758.32666015625\n",
            "[1100/2000] Loss:16463.3672\n",
            " Validation Loss: 3739.948486328125\n",
            "[1200/2000] Loss:16025.0469\n",
            " Validation Loss: 3729.6845703125\n",
            "[1300/2000] Loss:15984.5674\n",
            " Validation Loss: 3812.680419921875\n",
            "[1400/2000] Loss:14716.5967\n",
            " Validation Loss: 3723.379150390625\n",
            "Validation Loss Decreased(3724.510986--->3723.379150) \t Saving The Model\n",
            "[1500/2000] Loss:14603.7266\n",
            " Validation Loss: 3751.2099609375\n",
            "[1600/2000] Loss:14936.8535\n",
            " Validation Loss: 3737.77880859375\n",
            "[1700/2000] Loss:14863.9258\n",
            " Validation Loss: 3755.6669921875\n",
            "[1800/2000] Loss:16427.4277\n",
            " Validation Loss: 3728.6298828125\n",
            "[1900/2000] Loss:16377.6260\n"
          ]
        }
      ],
      "source": [
        "from networkx.algorithms.centrality.degree_alg import out_degree_centrality\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "def train():\n",
        "    model = DrBC_model()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define optimizer.\n",
        "    # model = model.cuda()   \n",
        "    eval_graph = Graph(batch_size=2)\n",
        "    evsl_bc_value = eval_graph.calculate_bc()\n",
        "    min_valid_loss = np.inf\n",
        "    iternum = 2000\n",
        "    wandb.init(project=\"DrBC\")\n",
        "    for iter in range(iternum):\n",
        "      optimizer.zero_grad()\n",
        "      if iter % 200 == 0 :\n",
        "        graph = Graph(10)  \n",
        "        bc_value = graph.calculate_bc()\n",
        "      out = model(graph.node_degree(), graph.get_edge_index())\n",
        "      source_ids, target_ids = graph.get_pairs()\n",
        "      loss = loss_function(out, bc_value, source_ids, target_ids)\n",
        "      wandb.log({'pair_loss': loss, 'iter': iter}) # record trsining loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # validation\n",
        "      if iter % 100 == 0: \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          out = model(eval_graph.node_degree(), eval_graph.get_edge_index())\n",
        "        source_ids, target_ids = eval_graph.get_pairs()\n",
        "        vali_loss = loss_function(out, evsl_bc_value, source_ids, target_ids)   \n",
        "        print(f' Validation Loss: {vali_loss}')\n",
        "        wandb.log({'validation_loss': vali_loss, 'iter': iter})\n",
        "        if min_valid_loss > vali_loss:\n",
        "          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{vali_loss:.6f}) \\t Saving The Model')\n",
        "          min_valid_loss = vali_loss\n",
        "          # Saving State Dict\n",
        "          torch.save(model.state_dict(), 'saved_model.pth')       \n",
        "        print(\"[{}/{}] Loss:{:.4f}\".format(iter, iternum, loss.item()))             \n",
        "    return model\n",
        "\n",
        "model = train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing "
      ],
      "metadata": {
        "id": "zkcPsMiYxSwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx.algorithms.centrality import betweenness\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "import scipy.stats as stats # Kendall tau\n",
        "import random\n",
        "import math\n",
        "\n",
        "# connect to google drive\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "class test_data():\n",
        "  def __init__(self):\n",
        "    super(test_data, self).__init__()\n",
        "    self.TestGraph_feature =[]\n",
        "    self.data = []\n",
        "    self.edge_list = []\n",
        "    self.BC = [] \n",
        "    self.TestGraph_edge=[[],[]]# TestGraph_edge[0]=source,[1]= target\n",
        "    \n",
        "  def readfile(self,path):\n",
        "      data = []\n",
        "      file = open(path, \"r\")\n",
        "      for line in file.readlines():\n",
        "          data.append(line.split())\n",
        "      file.close()\n",
        "      return data\n",
        "\n",
        "  def get_num_node(self, score_path): \n",
        "    num_node = 0\n",
        "    # calculate node number\n",
        "    for i in score_path: \n",
        "      num_node+=1\n",
        "    print(num_node)\n",
        "    node_degree = [0] * num_node\n",
        "    return node_degree\n",
        "\n",
        "  def get_edge_node_index(self,edge_index,node_degree):\n",
        "    # initial node degree \n",
        "    start_edge=[]; end_edge=[]\n",
        "    print(np.shape(edge_index))\n",
        "    for [s, t] in edge_index:\n",
        "      # get node degree\n",
        "      node_degree[int(s)]+= 1\n",
        "      node_degree[int(t)] += 1\n",
        "      # bidirectional edge\n",
        "      start_edge += [int(s)]\n",
        "      end_edge += [int(t)]\n",
        "    # node extension : bidirectional edge\n",
        "    for nb in node_degree:\n",
        "      self.TestGraph_feature.append([nb, 1, 1])\n",
        "    self.TestGraph_edge[0].extend(start_edge)\n",
        "    self.TestGraph_edge[1].extend(end_edge)\n",
        "    self.TestGraph_edge[0].extend(end_edge)\n",
        "    self.TestGraph_edge[1].extend(start_edge)\n",
        "    return self.TestGraph_edge, self.TestGraph_feature\n",
        "  \n",
        "  # betweenness centrality value\n",
        "  def get_BC(self,file_path): \n",
        "    gt = self.readfile(file_path)\n",
        "    for (node_id, bc) in gt:\n",
        "      bc = -math.log(float(bc)+1e-8)\n",
        "      self.BC.append([bc])\n",
        "    return self.BC\n",
        "\n",
        "  # Top-N % accuracy\n",
        "  def takeSecond(self,elem):\n",
        "      return elem[1]\n",
        "\n",
        "  def topN_accuracy(self,file,outs,n,bc_value):\n",
        "    predict_value,bc_value = [],[]\n",
        "    for i,j in enumerate(outs.tolist()):\n",
        "      predict_value.append([i,*j])\n",
        "    bc_value.sort(key = self.takeSecond,reverse = True)\n",
        "    predict_value.sort(key = self.takeSecond,reverse = True)\n",
        "    p,t = [],[]\n",
        "    for x in range(int(len(predict_value)*n/100)):\n",
        "      p.append(predict_value[x][0])\n",
        "      t.append(bc_value[x][0])\n",
        "    return(len(set(t)&set(p)) / len(p))\n",
        "\n",
        "  def kendall_tau(ground_true,outs,bc_value):\n",
        "    predict_value,bc_value = [],[]\n",
        "    for i,j in enumerate(outs.tolist()):\n",
        "      predict_value.append(*j)\n",
        "    for i in ground_true:\n",
        "      bc_value.append(i[1])\n",
        "    # print(predict_value)\n",
        "    # print(bc_value)\n",
        "    tau, _ = stats.kendalltau(predict_value, bc_value)\n",
        "    return(tau)\n",
        "    print(kendall_tau(f,outs))\n",
        "\n",
        "\n",
        "def testing(PATH, Ground_truthPath):\n",
        "  testing = test_data()\n",
        "  model = DrBC_model()\n",
        "  # model = torch.load('/content/drive/MyDrive/dataset/model/saved_model.pth')\n",
        "  model.load_state_dict(torch.load('/content/drive/MyDrive/dataset/model/saved_model.pth'))\n",
        "  predictbc_value = testing.get_BC(PATH)\n",
        "  ground_trueBC = testing.get_BC(Ground_truthPath)\n",
        "  # model.cuda()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # ground truth \n",
        "    dataset = testing.readfile(PATH)\n",
        "    dataset_score = testing.readfile(Ground_truthPath)\n",
        "    node_degree_data = testing.get_num_node(dataset_score)\n",
        "    TestGraph_edge_index , TestGraph_feature = testing.get_edge_node_index(dataset, node_degree_data)\n",
        "    # model = model.cpu()\n",
        "    outs = model(torch.FloatTensor(TestGraph_feature), torch.tensor(TestGraph_edge_index))\n",
        "    print(testing.topN_accuracy(PATH,outs,1,predictbc_value))\n",
        "    print(testing.topN_accuracy(PATH,outs,5,predictbc_value))\n",
        "    print(testing.topN_accuracy(PATH,outs,10,predictbc_value))\n",
        "    testing.kendall_tau(ground_trueBC,outs,predictbc_value)\n",
        "\n",
        "# file path\n",
        "yt_node_pair_file =  '/content/drive/MyDrive/dataset/com-youtube.txt'\n",
        "yt_bc_score_file = '/content/drive/MyDrive/dataset/com-youtube_score.txt'\n",
        "Synthetic_graph = \"/content/drive/MyDrive/dataset/Synthetic_graph.txt\"\n",
        "Synthetic_score = \"/content/drive/MyDrive/dataset/Synthetic_score.txt\"\n",
        "# youtube graph \n",
        "testing(yt_node_pair_file,yt_bc_score_file)   \n",
        "# Synthetic graph\n",
        "testing(Synthetic_graph,Synthetic_score)   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXatl4L4J4cI",
        "outputId": "f2f7f357-8c36-41e5-c149-40aecca33763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "1134890\n",
            "(2987624, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "Gl0qXJdaMxQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# root_path = \"D:/qiongyun/desktop/Master/GNN/hw1_data/Synthetic/5000/\"\n",
        "# f = \"D:/qiongyun/desktop/Master/GNN/hw1_data/Synthetic/Synthetic_graph.txt\"\n",
        "# f2 = \"D:/qiongyun/desktop/Master/GNN/hw1_data/Synthetic/Synthetic_score.txt\"\n",
        "\n",
        "# # Synthetic graph\n",
        "# file = open(f,\"a\") #append mode\n",
        "# file_2 = open(f2,\"a\") #append mode\n",
        "# for id in range(29):\n",
        "#     file1 = open(os.path.join(root_path, str(id) + '.txt'))\n",
        "#     file2 = open(os.path.join(root_path, str(id) + '_score.txt'))\n",
        "#     file.write(file1.read())\n",
        "#     file_2.write(file2.read())\n",
        "# file.close()\n",
        "# file2.close()"
      ],
      "metadata": {
        "id": "ETZgFxwAxIgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment 1 : Learning to Identify High BC Nodes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}